{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b12d6772",
   "metadata": {},
   "source": [
    "## 1. Import Libraries and Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9059ef8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import comprehensive libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Model loading and metrics\n",
    "import pickle\n",
    "import joblib\n",
    "import json\n",
    "import tensorflow as tf\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, silhouette_score\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "import sys\n",
    "import os\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.abspath('__file__'))))\n",
    "\n",
    "from src.exception import CustomException\n",
    "from src.logger import logging\n",
    "\n",
    "# Set plotting style\n",
    "plt.style.use('seaborn-v0_8')\n",
    "sns.set_palette(\"husl\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "print(\"üöÄ COMPREHENSIVE MODEL COMPARISON ANALYSIS\")\n",
    "print(\"üìä Evaluating: Traditional ML | Deep Learning ANNs | Customer Segmentation\")\n",
    "print(\"üéØ Goal: Production-ready model recommendation with business insights\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6a6cbae",
   "metadata": {},
   "source": [
    "## 2. Load Dataset and All Model Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0ad4358",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the original dataset\n",
    "df = pd.read_csv('../data/ecommerce_customer.csv')\n",
    "print(f\"üìã Original dataset shape: {df.shape}\")\n",
    "print(f\"üí∞ Target range: ${df['Yearly Amount Spent'].min():.2f} - ${df['Yearly Amount Spent'].max():.2f}\")\n",
    "print(f\"üìä Average spending: ${df['Yearly Amount Spent'].mean():.2f}\")\n",
    "\n",
    "# Initialize results storage\n",
    "all_results = {\n",
    "    'traditional_ml': {},\n",
    "    'deep_learning': {},\n",
    "    'clustering': {}\n",
    "}\n",
    "\n",
    "model_availability = []\n",
    "\n",
    "# Load Traditional ML results\n",
    "try:\n",
    "    with open('../models/regression/model_results.pkl', 'rb') as f:\n",
    "        all_results['traditional_ml'] = pickle.load(f)\n",
    "    print(\"‚úÖ Traditional ML results loaded\")\n",
    "    model_availability.append('Traditional ML')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Traditional ML results not found - run notebook 02_Regression_Models.ipynb first\")\n",
    "\n",
    "# Load Deep Learning results  \n",
    "try:\n",
    "    with open('../models/deep_learning/dl_results.json', 'r') as f:\n",
    "        dl_data = json.load(f)\n",
    "        all_results['deep_learning'] = dl_data['results']\n",
    "    print(\"‚úÖ Deep Learning results loaded\")\n",
    "    model_availability.append('Deep Learning')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Deep Learning results not found - run notebook 05_Deep_Learning_Regression.ipynb first\")\n",
    "\n",
    "# Load Clustering results\n",
    "try:\n",
    "    with open('../models/clustering/clustering_results.pkl', 'rb') as f:\n",
    "        clustering_data = pickle.load(f)\n",
    "        all_results['clustering'] = clustering_data['results']\n",
    "    print(\"‚úÖ Clustering results loaded\")\n",
    "    model_availability.append('Customer Segmentation')\n",
    "except FileNotFoundError:\n",
    "    print(\"‚ö†Ô∏è  Clustering results not found - run notebook 03_Customer_Segmentation.ipynb first\")\n",
    "\n",
    "print(f\"\\nüìà Available model categories: {', '.join(model_availability)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b89c1795",
   "metadata": {},
   "source": [
    "## 3. Performance Comparison Dashboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5928905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive comparison DataFrame\n",
    "comparison_data = []\n",
    "\n",
    "# Add Traditional ML results\n",
    "for model_name, metrics in all_results['traditional_ml'].items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Category': 'Traditional ML',\n",
    "        'Test R¬≤': metrics.get('test_r2', 0),\n",
    "        'Test RMSE': metrics.get('test_rmse', float('inf')),\n",
    "        'Test MAE': metrics.get('test_mae', float('inf')),\n",
    "        'Train R¬≤': metrics.get('train_r2', 0),\n",
    "        'Interpretability': 'High' if 'linear' in model_name.lower() else 'Medium',\n",
    "        'Training Speed': 'Fast',\n",
    "        'Prediction Speed': 'Very Fast'\n",
    "    })\n",
    "\n",
    "# Add Deep Learning results\n",
    "for model_name, metrics in all_results['deep_learning'].items():\n",
    "    comparison_data.append({\n",
    "        'Model': model_name,\n",
    "        'Category': 'Deep Learning',\n",
    "        'Test R¬≤': metrics.get('test_r2', 0),\n",
    "        'Test RMSE': metrics.get('test_rmse', float('inf')),\n",
    "        'Test MAE': metrics.get('test_mae', float('inf')),\n",
    "        'Train R¬≤': metrics.get('train_r2', 0),\n",
    "        'Interpretability': 'Low',\n",
    "        'Training Speed': 'Slow',\n",
    "        'Prediction Speed': 'Fast'\n",
    "    })\n",
    "\n",
    "# Create comparison DataFrame\n",
    "if comparison_data:\n",
    "    comparison_df = pd.DataFrame(comparison_data)\n",
    "    comparison_df = comparison_df.sort_values('Test R¬≤', ascending=False)\n",
    "    \n",
    "    print(\"üèÜ MODEL PERFORMANCE LEADERBOARD\")\n",
    "    print(\"=\"*60)\n",
    "    display(comparison_df.round(4))\n",
    "    \n",
    "    # Identify best models\n",
    "    best_overall = comparison_df.iloc[0]\n",
    "    best_traditional = comparison_df[comparison_df['Category'] == 'Traditional ML'].iloc[0] if 'Traditional ML' in comparison_df['Category'].values else None\n",
    "    best_dl = comparison_df[comparison_df['Category'] == 'Deep Learning'].iloc[0] if 'Deep Learning' in comparison_df['Category'].values else None\n",
    "    \n",
    "    print(f\"\\nü•á BEST OVERALL: {best_overall['Model']} ({best_overall['Category']})\")\n",
    "    print(f\"   R¬≤ Score: {best_overall['Test R¬≤']:.4f} | RMSE: {best_overall['Test RMSE']:.2f}\")\n",
    "    \n",
    "    if best_traditional is not None:\n",
    "        print(f\"\\nüèÖ BEST TRADITIONAL ML: {best_traditional['Model']}\")\n",
    "        print(f\"   R¬≤ Score: {best_traditional['Test R¬≤']:.4f} | RMSE: {best_traditional['Test RMSE']:.2f}\")\n",
    "    \n",
    "    if best_dl is not None:\n",
    "        print(f\"\\nü§ñ BEST DEEP LEARNING: {best_dl['Model']}\")\n",
    "        print(f\"   R¬≤ Score: {best_dl['Test R¬≤']:.4f} | RMSE: {best_dl['Test RMSE']:.2f}\")\n",
    "        \n",
    "else:\n",
    "    print(\"‚ö†Ô∏è  No regression model results available for comparison\")\n",
    "    print(\"   Run notebooks 02, 05 first to generate model results\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
